{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Techniques: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic text summarization is the task of producing a concise and fluent summary of a text while preserving key information content and overall meaning.\n",
    "\n",
    "The internet age has brought massive amounts of information, mostly in the form of non-structured textual data, but we don't have enough time to read it all. \n",
    "For this reason,  there is a need to develop ML algorithms that can automatically shorten longer texts and deliver accurate summaries that can fluently pass the intended messages.\n",
    "\n",
    "Applying automatic text summarization can:\n",
    "- Enhance the readability of documents\n",
    "- Reduce reading time \n",
    "- Accelerate the process of researching for information \n",
    "- Increase the amount of information that can fit in a particular area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches to automatic text summarization: \n",
    "- **Extractive** summarization systems aim to extract a subset of words, sentences or paragraphs which best represents a summary of the text. \n",
    " - Pros: They are quite robust to semantic inconsistencies since they use existing sentences that are taken straight from the input.\n",
    " - Cons: They lack in flexibility since they cannot use novel words or paraphrase.\n",
    "- **Abstractive** summarization systems aim to concisely paraphrase the content of the documents and generate a summary that captures the salient ideas of the source text. The generated summaries potentially contain new phrases and sentences that may not appear in the source text. \n",
    " - Pros: They can use words that were not in the original input. This enables to make more fluent and natural summaries.\n",
    " - Cons: It is a much harder NLP problem, still under active research.\n",
    "\n",
    "The great majority of existing approaches to automatic summarization are extractive, mostly because it is much easier to select text than it is to generate text from scratch.\n",
    "\n",
    "On the other hand, the extractive approach is too restrictive to produce human-like summaries – especially of longer, more complex text where just selecting and rearranging sentences is not enough.\n",
    "\n",
    "Therefore, abstractive summarization may be difficult, but it’s an essential field of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most widely used method for extractive text summarization consists in sentence extraction, in which the sentences contained in the text are ranked based on their relevance and the top N sentences are then selected and returned.\n",
    "\n",
    "Extractive summarization techniques can be divided into several categories. The most famous are the following:\n",
    "- **Graph-based** - This approach consists in building a graph having sentences as nodes and edges weighted by some notion of similarity between sentences. Then graph algorithms are applied to detect sentences that appear \"central\" to the document. The idea is that, if one sentence is very similar to many others, it will likely be a sentence of great importance. The importance of this sentence also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences.\n",
    "- **Feature-based** - This approach extracts the features of the sentence and then evaluate its importance based on its features. Several features can be considered, depending on the use case, such as:\n",
    " - Position of the sentence in the input document (e.g. the first sentence of a document usually is very important)\n",
    " - Presence of the verb in the sentence or some other Part of Speech (POS)\n",
    " - Length of the sentence\n",
    " - Term frequency (TF, TF-IDF)\n",
    " - Presence of named entity tags (NER)\n",
    "- **Topic-based** - This approach calculates the topics of the document and evaluates each sentence by how representative it is for any of these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples of extractive techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstraction-based summarization approaches must address a wide variety of NLP problems, such as natural language generation and semantic representation.\n",
    "\n",
    "In general, building abstract summaries is a challenging task, as it requires complicated deep learning techniques and sophisticated language modeling. Therefore, they are still far away from reaching human-level quality in summary generation, despite recent progress in the deep learning domain.\n",
    "\n",
    "The most common deep learning approaches to abstractive summarization are encoder-decoder architectures, from traditional RNNs to more recent transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder architecture with RNN and attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:summarizer_dev]",
   "language": "python",
   "name": "conda-env-summarizer_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
